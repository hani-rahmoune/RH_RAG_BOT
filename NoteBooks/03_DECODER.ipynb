{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzP-jlfuqGhR",
    "outputId": "522fe4ff-2ccb-4efb-cb61-363c5f171784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gAJCelVSp__f"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684,
     "referenced_widgets": [
      "a224806e748e46e4b557c30b75d6b638",
      "b6522eae5a74438ea97ff078da8641c0",
      "1ace16aa66d344e0985b963a35fc4c0c",
      "53e7cacc6bac4bc5b273792e4ea1d607",
      "35245545edbd4f55bb4b5980c8c295a1",
      "e6235aa6e57f40ab8585e34a62e6d642",
      "f89ce3b7adfb4c43b5e9becfb69b2eb0",
      "4c5a3050857f4b28b2cad71db2e0282a",
      "3faf564059244cc386cd274cc212e779",
      "5b90f46400b34850b39663bfdd7d54fc",
      "45e73d58f80e43d6b29082e4325e8aa4",
      "129478db33ea4f36bb11b74cb3e8b66a",
      "664ab050946a4247b6fed66704e494f9",
      "8c4dc1b47ba642c293fb1f2da638a29d",
      "499c015fc29648e5a2db22ddd14f4261",
      "4835e0bb3bf54dd5ac26d17f4b3088bc",
      "c816456a4e3040a0b465728ede39144a",
      "8396054462364d8dbdbd13157362586c",
      "1b874f36ac544baf810a4c1444d216f0",
      "8954033cb40b4c9aa25bf2afe12415be",
      "15e2830dda17494794574f204d741c32",
      "6de5b3b6227549b68e99c0cc4130b563",
      "53db986c98f14b0c9db4c66e5e929b75",
      "af8e457ab0c04f3e9fd2d7d0bf5103a0",
      "a07ac291e7754caab14bb4a7840c64e2",
      "c271cfaefa1c4b4694d8673658769788",
      "a8638c78b74b427085f1f529425ea394",
      "82fd6c2853524d6c8dbb9a7f7f380ecd",
      "6d13198e5c7548bfb13bf1c4d959a764",
      "e9ba0ffb74f840f18f170ec307e8fe52",
      "863efa5224fd415d979f3ace6608f8cf",
      "967b47858675405f845411477af2158f",
      "033cf93e5caf43859fef9a076a2b7939",
      "eac5a7911370415abb8be3aa0af1aa97",
      "4f7a8a0b969e43b7895a9fcb74084fe2",
      "8413eb1cadd04d19901d6597f8dfdd89",
      "a0efdd58c2284686889c3905c251298b",
      "f9f7299a67f04780bc2176d403e01074",
      "0f0da9d365ad4b13b73c5446a77cb663",
      "fd8eebc9752d4f1aafc93a360824c423",
      "9bef07e7b02f488a9c569eb93a6f2d88",
      "d44569d8eebb417f9589bb45b86500f4",
      "095e9bade289402b86d9db4dfbae52b8",
      "1b0e2daea4224598a5f74198c618c53c",
      "49f12302a8954c958d0b6bd0432f4451",
      "52f765e84fbe486cb13a178115a6fb16",
      "fb24269118ee4264b5473588fdc75ecc",
      "7952c7ecd10e4b829f258b939fd2d498",
      "4cdeb883ad0841edabc886a7bb0fa15e",
      "fe9326cc89524a2192b5561d1be8d011",
      "a9396e4b23e7443d89cfa45a340104b0",
      "73527d3b55b8410a8920c83860412a9e",
      "94f3ac24acdd42d0820afbfc2849130f",
      "2d8c892f99e343d98946c16fe6b264f9",
      "459fa7c7f1fc479a9582b18122b12d49",
      "3bf6cce5ec524b9d863f0630393e140d",
      "e18edc4a91ac48788336eaed5a694b23",
      "bcb2743e13ee49cbaee7f4789d842a5a",
      "ca87616942d745d78c7254ac759d4caa",
      "0a31eb6452944ff4831567ec22afa759",
      "ef989832987941abb959a50202e34bbd",
      "08546811560f4c0ea3d53d205f220e32",
      "60646ee039bc47d197f124cabf1f8cb3",
      "cefe5b80ff484c15883fd903c17c5042",
      "ca67284d7ae948afaa62dea518891335",
      "ff199887568b4307a9822514b657ad1c",
      "cd078e1cd8184e1c9e3178a1a0f8e749",
      "646d114ac6d3477a84033e6a50814ef8",
      "679f1aa5e7484f14b5fe6d0f6604637c",
      "7e894d34f726425193b0838921a83eda",
      "bfce65a6ed694a50862e827c87f709eb",
      "0b4d2d801c0346eb8aa11aa409b7dc0d",
      "52a347bbfd784dddbdc8bc8b7e5f5b9f",
      "21f543698a11451cb8a4c1f635f22cdf",
      "fa0643e263ff42c3b6c93bdc05b8fe98",
      "51eb9891e5634415b8ef4fafbca164b1",
      "8bdd932e930747528e359e4049c257d5",
      "8f5aa6db7f954ce38648906100c5fc2b",
      "c3c590c68566474189264475826c2686",
      "8bd0eb1de164464f9e1a2cec1020b887",
      "686f02d96f2647f08e4c5b3394a300e8",
      "da026b4e3b4a4c2e8b77e788fe0032b6",
      "b67f7b68117a47e1a730774eb8a7c3cf",
      "09b14c163d064d66879fec0c753dac7d",
      "971b534146b0456b87576d01884f62f0",
      "eee5cb1efeea42d39f179814e4e6af70",
      "871303f4bca64a7a9a333a04d843b917",
      "44c241089e0946ae804f0d531c39726f",
      "e492b139c6aa47258c8bc62f1b7816ea",
      "360362b0c9fb421d89970de906456252",
      "fb21f3d9589244d2941350dd0067d60b",
      "edfb0b5e12c14555bc000c277686784b",
      "e5a1c399d2904190b3f89e216adff726",
      "a3289bcde6b640879d131485cb8e030e",
      "bc4e8c80696f48049f357828e2f6873b",
      "7eedff3121384d21b1c220372bab102e",
      "cceecb27d4bd4a5ca77912f2b77576c1",
      "9f79e9cd02c142e38f0f509057b3f2bf",
      "11af150045184e969a200b9056541b47",
      "2587a272ffc0457ca8eed2fbb1ae8661",
      "bc5685a30bcf4a599d5839cba26be99f",
      "acce98c92dc74874a94d2ecdfdc01242",
      "2b296937c2024a49a76e8bd4a734d79f",
      "8007467e60024444b1853eae8e5269e7",
      "51a5d50e172f473caef61322f490b301",
      "99f08313961540048a393801e3ccb8af",
      "32dc755e0be04d6eb8e17e1227809ef4",
      "339dc497674b43a6973bef5626754a32",
      "8320961f6db44e2e8299eebb63fae37c",
      "4ee8615cacf840e490a0a7cc3cac5f46",
      "448ad78082e3450e96e80b2a0c03c9e0",
      "47eddb78f6d24c9c8e0ade2493bc0b13",
      "9ad4f19b85364928be1a4b3971381504",
      "cb132f9fde1e42eba7ad45252758c849",
      "8eeec1e2e2e34003a1fb7256d489f87e",
      "2e5ef83fbd514e5da197c499d00901f0",
      "f13ee0400ae144af830518a1e83139e1",
      "2197e911b4654d93bbee7da7b1208da1",
      "8c3b199986844bc7ba2e17de2c1b4f94",
      "bd01329371f249449e288e4e9fe32312",
      "9c7f989601534d41b9065b4623e121a1"
     ]
    },
    "id": "oHnQv_UsqOTB",
    "outputId": "0695977a-4c4f-42f7-b540-ea5bd6b600e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size: 1425\n",
      "Chunks: 1425\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"/content/drive/MyDrive/projects/chu_chat_bot/data/index/index.faiss\")\n",
    "\n",
    "with open(\"/content/drive/MyDrive/projects/chu_chat_bot/data/index/chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "print(\"Index size:\", index.ntotal)\n",
    "print(\"Chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J2xJds9aq1dz"
   },
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k_retrieve=10):\n",
    "    q = model.encode([query], normalize_embeddings=True)\n",
    "    q = np.asarray(q, dtype=np.float32)\n",
    "\n",
    "    scores, idxs = index.search(q, top_k_retrieve)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        c = chunks[idx]\n",
    "        results.append({\"score\": float(score), \"chunk\": c})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_ZxUQbXFr2hX"
   },
   "outputs": [],
   "source": [
    "def select_context(results, sim_threshold=0.50, top_k=4):\n",
    "    # filter by similarity threshold\n",
    "    filtered = [r for r in results if r[\"score\"] >= sim_threshold]\n",
    "\n",
    "    if not filtered:\n",
    "        return [], 0.0\n",
    "\n",
    "    # limit chunks per source\n",
    "    out = []\n",
    "    per_source = {}\n",
    "\n",
    "    for r in sorted(filtered, key=lambda x: x[\"score\"], reverse=True):\n",
    "        src = r[\"chunk\"][\"source\"]\n",
    "        per_source[src] = per_source.get(src, 0)\n",
    "        if per_source[src] >= 2:\n",
    "            continue\n",
    "        out.append(r)\n",
    "        per_source[src] += 1\n",
    "        if len(out) >= top_k:\n",
    "            break\n",
    "\n",
    "    confidence = out[0][\"score\"] if out else 0.0\n",
    "    return out, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wB4MBRLOuFPk"
   },
   "outputs": [],
   "source": [
    "def build_context_block(selected):\n",
    "    lines = []\n",
    "    for r in selected:\n",
    "        c = r[\"chunk\"]\n",
    "        page = c.get(\"page\", c.get(\"page\", None))\n",
    "        text = c[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        lines.append(f\"- Source: {c['source']} (page {page})\\n  Excerpt: {text}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "SYSTEM_RULES = \"\"\"You are an internal HR assistant.\n",
    "Rules:\n",
    "- Answer ONLY using the provided context excerpts.\n",
    "- If the context does not contain the answer, say you do not have enough information.\n",
    "- Do NOT invent policies or numbers.\n",
    "- Always provide sources at the end in this format:\n",
    "  Sources:\n",
    "  - filename (page X)\n",
    "Use clear, concise French.\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(question, context_block):\n",
    "    return f\"\"\"{SYSTEM_RULES}\n",
    "\n",
    "Context excerpts:\n",
    "{context_block}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4ahNZ58w_gS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OgohVf-IvPny"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "def call_llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful HR assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2_j5VNJcyDRT"
   },
   "outputs": [],
   "source": [
    "PERSONAL_PATTERNS = [\n",
    "    \"mon solde\", \"mes congés restants\", \"mon salaire\", \"ma paie\", \"mon contrat\",\n",
    "    \"mon dossier\", \"mon planning\", \"mes heures\", \"mes bulletins\"\n",
    "]\n",
    "\n",
    "def is_personal_request(q):\n",
    "    ql = q.lower()\n",
    "    return any(p in ql for p in PERSONAL_PATTERNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eitr1y3SvdmB"
   },
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    if is_personal_request(question):\n",
    "      return {\n",
    "          \"answer\": \"Je ne peux pas accéder à vos informations personnelles (solde, paie, dossier). \"\n",
    "                    \"Veuillez consulter le SIRH / l’intranet RH ou contacter votre service RH.\",\n",
    "          \"sources\": [],\n",
    "          \"confidence\": 0.0,\n",
    "          \"refused\": True\n",
    "      }\n",
    "    results = retrieve(question)\n",
    "    selected, confidence = select_context(results)\n",
    "\n",
    "    if not selected:\n",
    "        return {\n",
    "            \"answer\": \"Je n’ai pas assez d’informations dans les documents disponibles pour répondre de façon fiable.\",\n",
    "            \"sources\": [],\n",
    "            \"confidence\": confidence,\n",
    "            \"refused\": True\n",
    "        }\n",
    "\n",
    "    context_block = build_context_block(selected)\n",
    "    prompt = build_prompt(question, context_block)\n",
    "\n",
    "    answer = call_llm(prompt)\n",
    "\n",
    "    sources = []\n",
    "    for r in selected:\n",
    "        c = r[\"chunk\"]\n",
    "        sources.append({\n",
    "            \"source\": c[\"source\"],\n",
    "            \"page\": c.get(\"page_start\", c.get(\"page\", None))\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"confidence\": confidence,\n",
    "        \"refused\": False,\n",
    "        \"debug_prompt\": prompt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkaVRIn3v9-O",
    "outputId": "d1bbd377-1bb2-4aee-8b76-ed98b6edafb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Peut-on prendre plus de 6 semaines de congés consécutives ?\n",
      "Confidence: 0.7506436109542847 Refused: False\n",
      "Sources: [{'source': 'reglement-interieur.pdf', 'page': 212}, {'source': 'RI_UA.pdf', 'page': 70}, {'source': 'RI_UA.pdf', 'page': 68}, {'source': 'reglement-interieur.pdf', 'page': 207}]\n",
      "Answer: Non, hors congés bonifiés, il n'est pas autorisé de prendre plus de 6 semaines consécutives de congés, sauf pour des raisons tenant au bon fonctionnement du service et après validation du chef de service. \n",
      "\n",
      "Sources:\n",
      "- RI_UA.pdf (page 70)\n",
      "\n",
      "Q: Quelles conditions pour un congé longue maladie ?\n",
      "Confidence: 0.6903470754623413 Refused: False\n",
      "Sources: [{'source': 'reglement-interieur.pdf', 'page': 246}, {'source': 'reglement-interieur.pdf', 'page': 241}, {'source': 'RI_UA.pdf', 'page': 74}, {'source': 'RI_UA.pdf', 'page': 68}]\n",
      "Answer: Pour bénéficier d'un congé longue maladie, la maladie doit :\n",
      "- mettre l’intéressé dans l’impossibilité d’exercer ses fonctions,\n",
      "- rendre nécessaire un traitement et des soins prolongés,\n",
      "- présenter un caractère invalidant et de gravité confirmée.\n",
      "\n",
      "De plus, l'ouverture du congé intervient après 6 mois d’arrêt maladie consécutifs à la demande soit de l'intéressé, soit du Directeur, puis le dossier est soumis au Comité Médical.\n",
      "\n",
      "Sources:\n",
      "- reglement-interieur.pdf (page 241)\n",
      "\n",
      "Q: Je veux connaître mon solde de congés, tu peux vérifier ?\n",
      "Confidence: 0.0 Refused: True\n",
      "Sources: []\n",
      "Answer: Je ne peux pas accéder à vos informations personnelles (solde, paie, dossier). Veuillez consulter le SIRH / l’intranet RH ou contacter votre service RH.\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"Peut-on prendre plus de 6 semaines de congés consécutives ?\",\n",
    "    \"Quelles conditions pour un congé longue maladie ?\",\n",
    "    \"Je veux connaître mon solde de congés, tu peux vérifier ?\"\n",
    "]\n",
    "\n",
    "for q in tests:\n",
    "    out = ask(q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"Confidence:\", out[\"confidence\"], \"Refused:\", out[\"refused\"])\n",
    "    print(\"Sources:\", out[\"sources\"])\n",
    "    print(\"Answer:\", out[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}